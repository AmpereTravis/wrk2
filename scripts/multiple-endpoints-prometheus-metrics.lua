-- This script extends wrk2 to handle multiple server addresses
-- as well as multiple paths (endpoints) per server
--
-- Intermediate status output is added in a format suitable for consumption via
-- the Prometheus push gateway

require "socket"

-- NOTE: This script requires a file "endpoints.lua" to be present in CWD.
-- "endpoints.lua" is generated by the wrapper script and contains the list
-- of endpoints to benchmark against. Generating a LUA file and including it
-- here significantly speeds up the thread start-up if many threads/ endpoints
-- are used.

-----------------
-- main() context

-- main() globals
local threads = {}
local counter = 1

function setup(thread)
    -- Fill global threads table with thread handles so done()
    -- can process per-thread data
    table.insert(threads, thread)
    thread:set("id",counter)
    counter = counter +1
end

-----------------
-- Thread context

function micro_ts()
    return 1000 * socket.gettime()
end

function xtract(str, match, default, err_msg)
    local ret, count = string.gsub(str, match, "%1", 1)
    if count == 0 then
        if not default then
            print(string.format("Error parsing URL '%s': %s",str,err_msg))
            os.exit(1)
        end
        ret = default
    end
    return ret
end

function prom(mname, value)
    return string.format(
                   "wrk2_benchmark_%s{thread=\"thread-%s\"} %d\n",mname,id,value)
end

function write_metrics(req, resp, avg, curr, avg_reconn, curr_reconn)
    w = prom("requests", req) .. prom("responses", resp) 
    w = w .. prom("average_rps", avg) .. prom("current_rps", curr)
    w = w .. prom("average_tcp_reconnect_rate", avg_reconn)
    w = w .. prom("run_average_tcp_reconnect_rate", avg_reconn)
    w = w .. prom("current_tcp_reconnect_rate", curr_reconn)
    f=io.open(string.format("thread-%d_seq-%d.txt", id, write_iter), "w+")
    f:write(w)
    f:flush()
    f:close()
    write_iter = write_iter + 1
end

function init(args)

    -- load pre-generated array of list of endpoints
    require "endpoints"

    -- Thread globals used by done()
    url_call_count = ""

    -- URL list variables
    --   Thread globals used by request(), response()
    addrs = {}
    idx = 0
    --   table of lists; per entry:
    --     proto, host, hostaddr, port, path + params
    endpoints={}
    --    table of prepared HTTP requests for endpoints above
    reqs={}

    -- reporting variables
    report_every=1 --seconds
    responses=0
    requests=0
    reconnects=0
    reconnects=0
    prev_reconnects=0
    start_msec = micro_ts()
    prev_msec = start_msec
    prev_call_count = 0
    print_report=0
    math.randomseed(start_msec)

    -- write first metric - all 0, to reset counters
    write_iter = 0
    write_metrics(0,0,0,0,0,0)

    -- parse command line URLs and prepare requests
    local prev_srv = {}
    for i,e in pairs(input_endpoints) do
        -- note that URL parsing does not support user/pass as 
        -- wrk2 does not support auth
        local proto = xtract(e,
                "^(http[s]?)://.*", nil, "missing or unsupported  protocol")
        local host  = xtract(
                e, "^http[s]?://([^/:]+)[:/]?.*", nil, "missing host")

        if proto == "http" then
            def_port=80
        else if proto == "https" then
                def_port=443
            else
                print(string.format("Unsupported protocol '%s'",proto))
                os.exit(1)
            end
        end
        local port  = xtract(e, "^http[s]?://[^/:]+:(%d+).*", def_port)
        local path  = xtract(e, "^http[s]?://[^/]+(/.*)","/")

        -- get IP addr(s) from hostname, validate by connecting
        -- only connect if we did not connect before to prevent thread init
        -- stampede
        local addr = nil
        if      prev_srv[0] == proto
            and prev_srv[1] == host
            and prev_srv[3] == port then

                addr = prev_srv[2]
            else
                for k, v in ipairs(wrk.lookup(host, port)) do
                    if wrk.connect(v) then
                        addr = v
                        break
                    end
                end
                if not addr then
                    print(string.format(
                        "Error: Unable to connect to %s port %s.", host, port))
                    os.exit(2)
                end
        end

        -- store the endpoint
        endpoints[i] = {}
        endpoints[i][0] = proto
        endpoints[i][1] = host
        endpoints[i][2] = addr
        endpoints[i][3] = port
        endpoints[i][4] = path
        endpoints[i][5] = string.format(
                    "GET %s HTTP/1.1\r\nHost:%s:%s\r\n\r\n", path, host, port)
        endpoints[i][6] = string.format(host) -- for reconnect comparison
                                              -- (regex objects aren't comparable)
        endpoints[i][7] = 0
        endpoints[i][8] = e
        prev_srv = endpoints[i]
    end

    input_endpoints=nil
    collectgarbage()

    -- initialize idx, assign req and addr
    idx = 0 -- math.random(0, #endpoints)
    wrk.thread.addr = endpoints[idx][2]
end

function request()
    local ret = endpoints[idx][5]
    requests = requests + 1
    return ret
end

function response(status, headers)

    -- Pick the next endpoint in the list of endpoints, for the next request
    -- Also, update the thread's remote server addr if endpoint
    -- is on a different server.

    local prev_srv = endpoints[idx][6]
    endpoints[idx][7] = endpoints[idx][7] + 1
    idx = (idx + 1) % (#endpoints + 1)

    if prev_srv ~= endpoints[idx][6] then
        -- Re-setting the thread's server address forces a reconnect
        wrk.thread.addr = endpoints[idx][2]
        reconnects = reconnects + 1
    end

    -- write out report and update callend endpoints string in configured
    -- interval
    responses = responses + 1
    local now_msec = micro_ts()
    if (now_msec - prev_msec) > report_every * 1000 then
        local diff_msec = now_msec - prev_msec
        local sdiff_msec = now_msec - start_msec

        write_metrics(requests, responses,
                      responses / (sdiff_msec / 1000),
                      (responses - prev_call_count) / (diff_msec / 1000),
                      reconnects / (sdiff_msec / 1000),
                      (reconnects-prev_reconnects) / (diff_msec / 1000))

        prev_reconnects = reconnects
        prev_msec = now_msec
        prev_call_count = responses
   end
end


function teardown()
    local now_msec = micro_ts()
    local called_arr = {}

    local diff_msec = now_msec - prev_msec
    local sdiff_msec = now_msec - start_msec

    write_metrics(requests, responses,
                  responses / (sdiff_msec / 1000),
                  (responses - prev_call_count) / (diff_msec / 1000),
                  reconnects / (sdiff_msec / 1000),
                  (reconnects-prev_reconnects) / (diff_msec / 1000))

    for i=0, #endpoints, 1 do
        called_arr[i+1] = endpoints[i][8] .. " " .. endpoints[i][7]
    end
    url_call_count = table.concat(called_arr, ";")
end

-----------------
-- main() context

function done(summary, latency, requests)
    print(string.format("Total Requests: %d", summary.requests))
    print(string.format("HTTP errors: %d", summary.errors.status))
    print(string.format("Requests timed out: %d", summary.errors.timeout)) 
    print(string.format("Bytes received: %d", summary.bytes))
    print(string.format("Socket connect errors: %d", summary.errors.connect))
    print(string.format("Socket read errors: %d", summary.errors.read))
    print(string.format("Socket write errors: %d", summary.errors.write))

    -- extract individual URL call counts from threads, sum up
    local t = unpack(threads,1,2)
    local urls = {}

    for i, t in ipairs(threads) do
        local url_call_count = t:get("url_call_count")
        for entry in string.gmatch(url_call_count, "([^;]+)") do
            local url = string.match(entry,"^([^ ]+) .*")
            local count = string.match(entry,".* ([^ ]+)")
            if urls[url] then
                urls[url] = urls[url] + count
            else
                urls[url] = count
            end
        end
    end

    print("\nURL call count")
    for url, count in pairs(urls) do
        print(string.format("%s : %d", url, count))
    end
end
